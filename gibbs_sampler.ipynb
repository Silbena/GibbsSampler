{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAD2- Project 1\n",
    "Maria Nizik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 \n",
    "Joint probabilities:  \n",
    "\n",
    "$P(E=T, F=T, J=T, M=T) = P(E=T) \\cdot P(F=T|E=T) \\cdot (J=T|E=T) \\cdot P(M=T|F=T,J=T) = 0.5 \\cdot 0.1 \\cdot 0.8 \\cdot 0.95 = 0.038$  \n",
    "\n",
    "$P(E=F, F=T, J=T, M=T) = (1 - P(E=T)) \\cdot P(F=T|E=F) \\cdot (J=T|E=F) \\cdot P(M=T|F=T,J=T) = 0.5 \\cdot 0.5 \\cdot 0.1 \\cdot 0.95 = 0.02375$  \n",
    "\n",
    "$P(E=T, F=F, J=T, M=T) = P(E=T) \\cdot (1 - P(F=T|E=T)) \\cdot (J=T|E=T) \\cdot P(M=T|F=F,J=T) = 0.5 \\cdot 0.9 \\cdot 0.8 \\cdot 0.9 = 0.324$  \n",
    "\n",
    "$P(E=F, F=F, J=T, M=T) = (1 - P(E=F)) \\cdot (1 - P(F=T|E=F)) \\cdot (J=T|E=F) \\cdot P(M=T|F=F,J=T) = 0.5 \\cdot 0.5 \\cdot 0.1 \\cdot 0.9 = 0.0225$\n",
    "\n",
    "Conditional probabilities:  \n",
    "\n",
    "$P(E=T | F=T, J=T, M=T) = \\frac{P(E=T, F=T, J=T, M=T)}{\\sum_E P(E, F=T, J=T, M=T)} = \\frac{P(E=T, F=T, J=T, M=T)}{P(E=T, F=T, J=T, M=T) + P(E=F, F=T, J=T, M=T)} = \\frac{0.038}{0.038 + 0.02375} \\approx 0.615$ \n",
    "\n",
    "$P(E=T | F=F, J=T, M=T) = \\frac{P(E=T, F=F, J=T, M=T)}{\\sum_E P(E, F=F, J=T, M=T)} = \\frac{P(E=T, F=F, J=T, M=T)}{P(E=T, F=F, J=T, M=T)+P(E=F, F=F, J=T, M=T)} = \\frac{0.324}{0.324+0.0225} \\approx 0.935$  \n",
    "\n",
    "$P(F=T | E=T, J=T, M=T) = \\frac{P(E=T, F=T, J=T, M=T)}{\\sum_F P(F, E=T, J=T, M=T)} = \\frac{P(E=T, F=T, J=T, M=T)}{P(E=T, F=T, J=T, M=T) + P(E=T, F=F, J=T, M=T)} = \\frac{0.038}{0.038 + 0.324} \\approx 0.105$  \n",
    "\n",
    "$P(F=T | E=F, J=T, M=T) = \\frac{P(E=F, F=T, J=T, M=T)}{\\sum_F P(F, E=F, J=T, M=T)} = \\frac{P(E=F, F=T, J=T, M=T)}{P(E=F, F=T, J=T, M=T)+P(E=F, F=F, J=T, M=T)} = \\frac{0.02375}{0.02375+0.0225} \\approx 0.514$  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    \"\"\"Mortgage model\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"P(X=F) are included. \n",
    "        Example: [[P(False|False), P(False|True)], [P(True|False), P(True|True)]]\"\"\"\n",
    "        self.p_e = np.array([0.5, 0.5])\n",
    "\n",
    "        self.p_f_on_e = np.array([[0.5, 0.9],\n",
    "                               [0.5, 0.1]])\n",
    "        \n",
    "        self.p_j_on_e = np.array([[0.9, 0.2],\n",
    "                               [0.1, 0.8]])\n",
    "        \n",
    "        self.p_m_on_fj = np.array([[[0.9, 0.1],\n",
    "                                 [0.1, 0.05]],\n",
    "                                 [[0.1, 0.9],\n",
    "                                 [0.9, 0.95]]])\n",
    "\n",
    "    def count_conditional(self, variable: list, states: np.array) -> tuple[np.array]:\n",
    "        e, f, j, m = states\n",
    "\n",
    "        nominator = self.p_e[e] * self.p_f_on_e[f][e] * self.p_j_on_e[j][e] * self.p_m_on_fj[m][f][j]\n",
    "\n",
    "        states[variable[0]] ^= 1\n",
    "        e, f, _, _ = states\n",
    "\n",
    "        denominator = nominator + self.p_e[e] * self.p_f_on_e[f][e] * self.p_j_on_e[j][e] * self.p_m_on_fj[m][f][j]\n",
    "        \n",
    "        return nominator / denominator\n",
    "    \n",
    "model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6153846153846154 0.935064935064935 0.10497237569060772 0.5135135135135135\n",
      "0.3846153846153846 0.06493506493506493 0.8950276243093922 0.48648648648648657\n"
     ]
    }
   ],
   "source": [
    "# Model tests\n",
    "first = model.count_conditional([0, 'e'], np.array([1,1,1,1]))\n",
    "second = model.count_conditional([0, 'e'], np.array([1,0,1,1]))\n",
    "third = model.count_conditional([1, 'f'], np.array([1,1,1,1]))\n",
    "fourth = model.count_conditional([1, 'f'], np.array([0,1,1,1]))\n",
    "print(first, second, third, fourth)\n",
    "\n",
    "fi = model.count_conditional([0, 'e'], np.array([0,1,1,1]))\n",
    "s = model.count_conditional([0, 'e'], np.array([0,0,1,1]))\n",
    "t = model.count_conditional([1, 'f'], np.array([1,0,1,1]))\n",
    "fo = model.count_conditional([1, 'f'], np.array([0,0,1,1]))\n",
    "print(fi, s, t, fo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampler:\n",
    "    \"\"\"Gibbs sampler\"\"\"\n",
    "    def __init__(self, model: Model, environment: int = 1, funding: int = 1, job: int = 1, mortgage: int = 1):\n",
    "        self.model = model\n",
    "        self.e = environment\n",
    "        self.f = funding\n",
    "        self.j = job\n",
    "        self.m =  mortgage\n",
    "\n",
    "    def states(self) -> np.array:\n",
    "        return np.array([self.e, self.f, self.j, self.m])  \n",
    "            \n",
    "    def sample(self, sampling_iters: int = 1, burn_in_iters:int = 0, thin_by: int = 1) -> np.array:\n",
    "        \"\"\"Thin_by takes thining-out parameter. For thin = 1 there is no thinning and all samples are given.\"\"\"\n",
    "\n",
    "        iters = burn_in_iters + thin_by * sampling_iters\n",
    "        samples = []\n",
    "\n",
    "        for _ in range(iters):\n",
    "            i = np.random.choice([0,1])\n",
    "            variable = ([1, 'f'] if i else [0, 'e'])\n",
    "            prob = self.model.count_conditional(variable, self.states())\n",
    "\n",
    "            current_state = getattr(self, variable[1])\n",
    "            opposite_state = current_state ^ 1\n",
    "            new_state = np.random.choice([current_state, opposite_state], p = [prob, 1 - prob])\n",
    "\n",
    "            setattr(self, variable[1], new_state)\n",
    "            samples.append(self.states())\n",
    "\n",
    "        return np.array(samples[burn_in_iters::thin_by])\n",
    "    \n",
    "\n",
    "np.random.seed(123)\n",
    "sampler = Sampler(model) \n",
    "samples = sampler.sample(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(F = T|J = T, M = T) = 0.12\n"
     ]
    }
   ],
   "source": [
    "print('P(F = T|J = T, M = T) =', np.mean(samples[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(100)\n",
    "sampler2 = Sampler(model) \n",
    "samples2 = sampler2.sample(50000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third run of a sampler\n",
    "np.random.seed(51)\n",
    "samler3 = Sampler(model, environment=0, funding=0)\n",
    "samples3 = sampler.sample(50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iteration</th>\n",
       "      <th>variable</th>\n",
       "      <th>frequency</th>\n",
       "      <th>sampler</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>E</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>E</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>E</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>E</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>E</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199995</th>\n",
       "      <td>49995</td>\n",
       "      <td>F</td>\n",
       "      <td>0.148232</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199996</th>\n",
       "      <td>49996</td>\n",
       "      <td>F</td>\n",
       "      <td>0.148229</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199997</th>\n",
       "      <td>49997</td>\n",
       "      <td>F</td>\n",
       "      <td>0.148226</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199998</th>\n",
       "      <td>49998</td>\n",
       "      <td>F</td>\n",
       "      <td>0.148223</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199999</th>\n",
       "      <td>49999</td>\n",
       "      <td>F</td>\n",
       "      <td>0.148220</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        iteration variable  frequency  sampler\n",
       "0               0        E   0.000000        2\n",
       "1               1        E   0.000000        2\n",
       "2               2        E   0.000000        2\n",
       "3               3        E   0.000000        2\n",
       "4               4        E   0.000000        2\n",
       "...           ...      ...        ...      ...\n",
       "199995      49995        F   0.148232        3\n",
       "199996      49996        F   0.148229        3\n",
       "199997      49997        F   0.148226        3\n",
       "199998      49998        F   0.148223        3\n",
       "199999      49999        F   0.148220        3\n",
       "\n",
       "[200000 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def count_frequencies(sample_arrays: np.ndarray, cols_of_interest: list[int]) -> np.ndarray:\n",
    "    \"\"\"Computes relative frequencies and converts arrays of samples to easy-to-plot dataframes\"\"\"\n",
    "    \n",
    "    assert isinstance(sample_arrays, np.ndarray)\n",
    "\n",
    "    sample_arrays = sample_arrays[ : , : , cols_of_interest]\n",
    "    index_array = np.arange(1, len(sample_arrays[0]) + 1)[ : , np.newaxis]\n",
    "    means = np.cumsum(sample_arrays, axis = 1) / index_array\n",
    "    \n",
    "    return means\n",
    "\n",
    "\n",
    "def stats_to_df(data: np.ndarray, stat_name: str, sampler_nr: int) -> pd.DataFrame:\n",
    "    \"\"\"Converts stats array to easy-to-plot dataframe.\"\"\"\n",
    "\n",
    "    assert isinstance(data, np.ndarray)\n",
    "    assert data.ndim == 2\n",
    "\n",
    "    df = pd.DataFrame(data, columns=['E', 'F'])\n",
    "    df['iteration'] = df.index\n",
    "    df = df.melt(id_vars=['iteration'], value_name=stat_name)\n",
    "    df['sampler'] = sampler_nr\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def stats_to_combined_df(stats_arrays: np.ndarray, stat_name: str, sampler_nrs: list) -> pd.DataFrame:\n",
    "\n",
    "    assert isinstance(stats_arrays, np.ndarray)\n",
    "    assert stats_arrays.ndim == 3\n",
    "\n",
    "    dfs = []\n",
    "\n",
    "    for array, nr in zip(stats_arrays, sampler_nrs):\n",
    "        df = stats_to_df(array, stat_name, nr)\n",
    "        dfs.append(df)\n",
    "\n",
    "    combined = pd.concat(dfs, ignore_index = True) \n",
    "\n",
    "    return combined\n",
    "\n",
    "\n",
    "frequencies = count_frequencies(np.array([samples2, samples3]), [0, 1])\n",
    "combined_frequencies_df = stats_to_combined_df(frequencies, 'frequency', [2, 3])\n",
    "display(combined_frequencies_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequencies_plot(df: pd.DataFrame):\n",
    "\n",
    "    assert isinstance(df, pd.DataFrame)\n",
    "\n",
    "    plt = px.line(df,\n",
    "                x = 'iteration',\n",
    "                y = 'frequency',\n",
    "                facet_col = 'variable',\n",
    "                color = 'sampler',\n",
    "                title =  'E and F relative frequncies as Gibbs samplers progress',\n",
    "                height = 500,\n",
    "                width = 1000)\n",
    "    \n",
    "    plt.write_image('frequencies.png', scale = 8)\n",
    "\n",
    "\n",
    "frequencies_plot(combined_frequencies_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Plot](./frequencies.png)\n",
    "\n",
    "E = T relative frequencies converge after ~40k iterations and F = T after ~34k iterations. I would use the higher of these values (40k) as a burn-in time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6  \n",
    "The next 2 sets of 50000 samples are prepared. Each is initialised separately and with different seed to ensure independence, while mantaining reproducibility. Seeds were chosen once and arbitrarily. Each of samplers was initialised with different E and F values and no burn-in time: sampler2 (E = T, F = T), sampler3 (E = F, F = F), sampler4 (E = T, F = F), and sampler5 (E = F, F = T)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(133)\n",
    "sampler4 = Sampler(model, environment=1, funding=0)\n",
    "samples4 = sampler4.sample(50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(17)\n",
    "sampler5 = Sampler(model, environment=0, funding=1)\n",
    "samples5 = sampler5.sample(50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gelman_test(sample_arrays: np.ndarray, cols_of_interest: list[int]) -> np.ndarray:\n",
    "    \"\"\"Gelman-Rubin statistic.\"\"\"\n",
    "\n",
    "    assert isinstance(sample_arrays, np.ndarray)\n",
    "    assert sample_arrays.shape[0] > 1\n",
    "\n",
    "    sample_arrays = sample_arrays[ : , : , cols_of_interest]\n",
    "    index_array = np.arange(1, len(sample_arrays[0]) + 1)[ : , np.newaxis]\n",
    "\n",
    "    means = np.cumsum(sample_arrays, axis = 1) / index_array\n",
    "    grand_means = np.mean(means, axis = 0)\n",
    "    between_vars = np.sum((means - grand_means) ** 2, axis = 0) / (len(sample_arrays) - 1)\n",
    "    within_vars = np.sum((sample_arrays - means) ** 2, axis = 0) / index_array\n",
    "    ws= np.mean(within_vars, axis = 0) / len(sample_arrays)\n",
    "\n",
    "    result = (ws + between_vars) / ws\n",
    "\n",
    "    return result\n",
    "\n",
    "gelman_samples = np.array([samples2, samples3, samples4, samples5])\n",
    "gelman_result  = gelman_test(gelman_samples, [0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iteration</th>\n",
       "      <th>variable</th>\n",
       "      <th>gelman</th>\n",
       "      <th>sampler</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>E</td>\n",
       "      <td>15575.076235</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>E</td>\n",
       "      <td>15575.076235</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>E</td>\n",
       "      <td>11681.557177</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>E</td>\n",
       "      <td>8518.072941</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>E</td>\n",
       "      <td>8722.482692</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>49995</td>\n",
       "      <td>F</td>\n",
       "      <td>1.451000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>49996</td>\n",
       "      <td>F</td>\n",
       "      <td>1.454012</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>49997</td>\n",
       "      <td>F</td>\n",
       "      <td>1.457035</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>49998</td>\n",
       "      <td>F</td>\n",
       "      <td>1.456206</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>49999</td>\n",
       "      <td>F</td>\n",
       "      <td>1.455388</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       iteration variable        gelman  sampler\n",
       "0              0        E  15575.076235        2\n",
       "1              1        E  15575.076235        2\n",
       "2              2        E  11681.557177        2\n",
       "3              3        E   8518.072941        2\n",
       "4              4        E   8722.482692        2\n",
       "...          ...      ...           ...      ...\n",
       "99995      49995        F      1.451000        2\n",
       "99996      49996        F      1.454012        2\n",
       "99997      49997        F      1.457035        2\n",
       "99998      49998        F      1.456206        2\n",
       "99999      49999        F      1.455388        2\n",
       "\n",
       "[100000 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gelman_df = stats_to_df(gelman_result, 'gelman', 2)\n",
    "display(gelman_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gelman_plot(df: pd.DataFrame):\n",
    "\n",
    "    assert isinstance(df, pd.DataFrame)\n",
    "\n",
    "    plt = px.line(df,\n",
    "                x = 'iteration',\n",
    "                y = 'gelman',\n",
    "                color = 'variable',\n",
    "                title = 'Gelman tests for E and F as Gibbs sampler progresses',\n",
    "                width = 1000,\n",
    "                height =  500)\n",
    "    \n",
    "    plt.update_layout(yaxis_type = 'log')\n",
    "    \n",
    "    plt.write_image('gelman.png', scale = 8)\n",
    "\n",
    "\n",
    "gelman_plot(gelman_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Plot](gelman.png)\n",
    "After around 40k iteration reach scale reduction factors (for both E and F) are close enough to 1 to assume stationarity of the distributions. I would suggest 40k burn-in time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iteration</th>\n",
       "      <th>variable</th>\n",
       "      <th>autocorrelation</th>\n",
       "      <th>sampler</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>E</td>\n",
       "      <td>0.556955</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>E</td>\n",
       "      <td>0.339864</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>E</td>\n",
       "      <td>0.214807</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>E</td>\n",
       "      <td>0.136357</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>E</td>\n",
       "      <td>0.093897</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>45</td>\n",
       "      <td>F</td>\n",
       "      <td>-0.001107</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>46</td>\n",
       "      <td>F</td>\n",
       "      <td>-0.003725</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>47</td>\n",
       "      <td>F</td>\n",
       "      <td>0.000732</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>48</td>\n",
       "      <td>F</td>\n",
       "      <td>-0.000041</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>49</td>\n",
       "      <td>F</td>\n",
       "      <td>-0.004044</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    iteration variable  autocorrelation  sampler\n",
       "0           0        E         0.556955        2\n",
       "1           1        E         0.339864        2\n",
       "2           2        E         0.214807        2\n",
       "3           3        E         0.136357        2\n",
       "4           4        E         0.093897        2\n",
       "..        ...      ...              ...      ...\n",
       "95         45        F        -0.001107        2\n",
       "96         46        F        -0.003725        2\n",
       "97         47        F         0.000732        2\n",
       "98         48        F        -0.000041        2\n",
       "99         49        F        -0.004044        2\n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def count_autocorrelations(samples: np.ndarray, cols_of_interest: list,  k_max: int = 50) -> np.ndarray:\n",
    "\n",
    "    assert isinstance(samples, np.ndarray)\n",
    "    assert samples.ndim == 2\n",
    "    assert all(isinstance(col, int) for col in cols_of_interest)\n",
    "    assert all(0 <= col < samples.shape[1] for col in cols_of_interest)\n",
    "    assert k_max < samples.shape[0]\n",
    "\n",
    "    samples = samples[ : , cols_of_interest]\n",
    "\n",
    "    means = np.mean(samples, axis=0)\n",
    "    variances = np.var(samples, axis=0)\n",
    "    autocorrelations = []\n",
    "\n",
    "    for k in range(1, k_max + 1):\n",
    "        covs = (samples[:-k, :] - means)*(samples[k:, :] - means)\n",
    "        cor = np.mean(covs, axis = 0) / variances\n",
    "        autocorrelations.append(cor)\n",
    "\n",
    "    return np.array(autocorrelations)\n",
    "\n",
    "\n",
    "autocorrelations = count_autocorrelations(samples2, [0, 1])\n",
    "autocorrelations_df = stats_to_df(autocorrelations, 'autocorrelation', 2)\n",
    "display(autocorrelations_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cor_plot(df: pd.DataFrame):\n",
    "\n",
    "    assert isinstance(df, pd.DataFrame)\n",
    "\n",
    "    plt = px.line(df,\n",
    "                x = 'iteration',\n",
    "                y = 'autocorrelation',\n",
    "                color = 'variable',\n",
    "                title = 'Autocorrelations between samples at lag = k',\n",
    "                width = 1000,\n",
    "                height = 500,\n",
    "                labels = {'iteration':'k'})\n",
    "    \n",
    "    plt.write_image('correlations.png', scale = 8)\n",
    "\n",
    "\n",
    "cor_plot(autocorrelations_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Plot](correlations.png)\n",
    "\n",
    "Autocorrelations oscilating over 0 at lag 15 and higher suggest that samples taken with such lags are independent. I would take the smallest of this lags (15) as a thinning-out parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(F = T|J = T, M = T) = 0.16\n"
     ]
    }
   ],
   "source": [
    "sampler_tuned = Sampler(model)\n",
    "samples_tuned = sampler_tuned.sample(100, 40000, 15)\n",
    "print('P(F = T|J = T, M = T) =', np.mean(samples_tuned[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value of $P(F = T|J = T, M=T)$ estimated on the tuned samples after 40000 burn-in and 15 thin-out is similar to the basic one obtained without burn-in an thinning-out. It is hard to say, whether it is a better estimate without a systematic analysis. Thus, I compare those two approaches sampling many times with independent samplers (with basic and tuned parameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_samplers(iters_compare: int, sampling_iters: int = 100, burn_in_iters: int = 0, thin_by: int = 1) -> np.ndarray:\n",
    "    \"\"\"Compares tuned sampling (with burn-in and thinning-out) to the basic sampling (without).\"\"\"\n",
    "\n",
    "    assert isinstance(iters_compare, int)\n",
    "\n",
    "    means = np.zeros((iters_compare, 2), dtype=float)\n",
    "    variances = np.zeros((iters_compare, 2), dtype=float)\n",
    "\n",
    "    for i in range(iters_compare):\n",
    "        basic_sampler = Sampler(model)\n",
    "        tuned_sampler = Sampler(model)\n",
    "\n",
    "        basic_samples = basic_sampler.sample(sampling_iters)[:, 1]\n",
    "        tuned_samples = tuned_sampler.sample(sampling_iters, burn_in_iters, thin_by)[:, 1]\n",
    "        \n",
    "        means[i][0] = np.mean(basic_samples)\n",
    "        means[i][1] = np.mean(tuned_samples)\n",
    "\n",
    "        variances[i][0] = np.var(basic_samples)\n",
    "        variances[i][1] = np.var(tuned_samples)\n",
    "\n",
    "    m = np.mean(means, axis = 0)\n",
    "    v = np.mean(variances, axis = 0)\n",
    "\n",
    "    return m, v\n",
    "\n",
    "\n",
    "iters_compare = 100\n",
    "sampling_iters = 500\n",
    "burn_in_iters = 40000\n",
    "thin_by = 15\n",
    "\n",
    "means, variances = compare_samplers(iters_compare, sampling_iters=sampling_iters, burn_in_iters=burn_in_iters, thin_by=thin_by)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean and variance of P(F = T|J = T, M = T) for no burn-in and thinning-out is: (0.157, 0.131)\n",
      "Mean and variance of P(F = T|J = T, M = T) for burn_in_iters=40000, and thin_by=15 is: (0.152, 0.128)\n"
     ]
    }
   ],
   "source": [
    "print(f'Mean and variance of P(F = T|J = T, M = T) for no burn-in and thinning-out is: {float(means[0].round(3)), float(variances[0].round(3))}')\n",
    "print(f'Mean and variance of P(F = T|J = T, M = T) for {burn_in_iters=}, and {thin_by=} is: {float(means[1].round(3)), float(variances[1].round(3))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Systematic comparison of basic and tuned sampler shows, that the P(F = T|J = T, M = T) vary. One can not say, if predictions of on of them are more accurate without computing the probability analytically. But, because the variance between estimates is still high, I would use a bigger sample set to estimate P(F = T|J = T, M = T) reliably."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9\n",
    "\n",
    "$P(E=T, F=T, J=T, M=T) = 0.038$  \n",
    "\n",
    "$P(E=F, F=T, J=T, M=T) = 0.02375 $  \n",
    "\n",
    "$P(E=T, F=F, J=T, M=T) = 0.324 $  \n",
    "\n",
    "$P(E=F, F=F, J=T, M=T) = 0.0225 $\n",
    "\n",
    "$P(F = T| J = T, M = T) = \n",
    "\\frac{P(F=T, J=T, M=T)}{P(J=T, M=T)} = \n",
    "\\frac{\\sum_E P(E, F=T, J=T, M=T)}{\\sum_{E,F} P(E, F, J=T, M=T)} = \n",
    "\\frac{P(E=T, F=T, J=T, M=T) + P(E=F,F=T, J=T, M=T)}{P(E=T, F=T, J=T, M=T) + P(E=F,F=T, J=T, M=T) + P(E=T, F=F, J=T, M=T) + P(E=F, F=F, J=T, M=T)} = \n",
    "\\frac{0.038 + 0.02375}{0.038 + 0.02375 + 0.324 + 0.0225} \\approx 0.151$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sampling estimates (mean = 0.152) obtained with tuned sampler are closer to the analytically calculated probability (0.151) that obtained with the basic sampler (mean = 0.160). That said, the chosen parameters helped in obtaining better estimate."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
